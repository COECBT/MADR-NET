{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugk-GjgwebEv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, BatchNormalization, Activation, add\n",
        "from keras.models import Model, model_from_json\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import ELU, LeakyReLU\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD6uC7PtNKUx",
        "outputId": "f4eca123-e9d4-49c3-fb61-1e5338e18c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2wa_8mCjwvB",
        "outputId": "5dcff9bd-1373-4874-fc89-908ee13d41af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 1924/2000 [27:58<02:19,  1.83s/it]"
          ]
        }
      ],
      "source": [
        "img_files = next(os.walk('/content/drive/MyDrive/Skin/skin/ISIC-2017_Training_Data/ISIC-2017_Training_Data'))[2]\n",
        "msk_files = next(os.walk('/content/drive/MyDrive/Skin/skin/ISIC-2017_Training_Part1_GroundTruth/ISIC-2017_Training_Part1_GroundTruth'))[2]\n",
        "\n",
        "img_files.sort()\n",
        "msk_files.sort()\n",
        "\n",
        "print(len(img_files))\n",
        "print(len(msk_files))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for img_fl in tqdm(img_files):\n",
        "    if(img_fl.split('.')[-1]=='jpg'):\n",
        "\n",
        "\n",
        "        img = cv2.imread('/content/drive/MyDrive/Skin/skin/ISIC-2017_Training_Data/ISIC-2017_Training_Data/{}'.format(img_fl), cv2.IMREAD_COLOR)\n",
        "        resized_img = cv2.resize(img,(128,128), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "        X.append(resized_img)\n",
        "\n",
        "        msk = cv2.imread('/content/drive/MyDrive/Skin/skin/ISIC-2017_Training_Part1_GroundTruth/ISIC-2017_Training_Part1_GroundTruth/{}'.format(img_fl.split('.')[0]+'_segmentation.png'), cv2.IMREAD_GRAYSCALE)\n",
        "        resized_msk = cv2.resize(msk,(128,128), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "        Y.append(resized_msk)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X))\n",
        "print(len(Y))\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "Y_train = Y_train.reshape((Y_train.shape[0],Y_train.shape[1],Y_train.shape[2],1))\n",
        "Y_test = Y_test.reshape((Y_test.shape[0],Y_test.shape[1],Y_test.shape[2],1))\n",
        "\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "Y_train = Y_train / 255\n",
        "Y_test = Y_test / 255\n",
        "\n",
        "Y_train = np.round(Y_train,0)\n",
        "Y_test = np.round(Y_test,0)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "id": "k_NqfgIjr3RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1, X_test_do, Y_train1, Y_test_d0 = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5kv7uNZp6T1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijrtd_NOhSv6"
      },
      "outputs": [],
      "source": [
        "print(X_train1.shape)\n",
        "print(Y_train1.shape)\n",
        "print(X_test_do.shape)\n",
        "print(Y_test_d0.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74-98cdshZ2w"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.utils.extmath import cartesian\n",
        "import math\n",
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 0.0\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def jacard(y_true, y_pred):\n",
        "\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum ( y_true_f * y_pred_f)\n",
        "    union = K.sum ( y_true_f + y_pred_f - y_true_f * y_pred_f)\n",
        "\n",
        "    return intersection/union\n",
        "\n",
        "def confusion(y_true, y_pred):\n",
        "    smooth=1\n",
        "    y_pred_pos = K.clip(y_pred, 0, 1)\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        "    y_pos = K.clip(y_true, 0, 1)\n",
        "    y_neg = 1 - y_pos\n",
        "    tp = K.sum(y_pos * y_pred_pos)\n",
        "    fp = K.sum(y_neg * y_pred_pos)\n",
        "    fn = K.sum(y_pos * y_pred_neg)\n",
        "    prec = (tp + smooth)/(tp+fp+smooth)\n",
        "    recall = (tp+smooth)/(tp+fn+smooth)\n",
        "    return prec, recall\n",
        "\n",
        "def tp(y_true, y_pred):\n",
        "    smooth = 1\n",
        "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "    tp = (K.sum(y_pos * y_pred_pos) + smooth)/ (K.sum(y_pos) + smooth)\n",
        "    return tp\n",
        "\n",
        "def tn(y_true, y_pred):\n",
        "    smooth = 1\n",
        "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "    y_neg = 1 - y_pos\n",
        "    tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth )\n",
        "    return tn\n",
        "def cdist(A, B):\n",
        "    \"\"\"\n",
        "    Computes the pairwise Euclidean distance matrix between two tensorflow matrices A & B, similiar to scikit-learn cdist.\n",
        "    For example:\n",
        "    A = [[1, 2],\n",
        "         [3, 4]]\n",
        "    B = [[1, 2],\n",
        "         [3, 4]]\n",
        "    should return:\n",
        "        [[0, 2.82],\n",
        "         [2.82, 0]]\n",
        "    :param A: m_a x n matrix\n",
        "    :param B: m_b x n matrix\n",
        "    :return: euclidean distance matrix (m_a x m_b)\n",
        "    \"\"\"\n",
        "    # squared norms of each row in A and B\n",
        "    na = tf.reduce_sum(tf.square(A), 1)\n",
        "    nb = tf.reduce_sum(tf.square(B), 1)\n",
        "\n",
        "    # na as a row and nb as a co\"lumn vectors\n",
        "    na = tf.reshape(na, [-1, 1])\n",
        "    nb = tf.reshape(nb, [1, -1])\n",
        "\n",
        "    # return pairwise euclidead difference matrix\n",
        "    D = tf.sqrt(tf.maximum(na - 2 * tf.matmul(A, B, False, True) + nb, 0.0))\n",
        "    return D\n",
        "\n",
        "\n",
        "def weighted_hausdorff_distance(w, h, alpha):\n",
        "    all_img_locations = tf.convert_to_tensor(cartesian([np.arange(w), np.arange(h)]), dtype=tf.float32)\n",
        "    max_dist = math.sqrt(w ** 2 + h ** 2)\n",
        "\n",
        "    def hausdorff_loss(y_true, y_pred):\n",
        "        def loss(y_true, y_pred):\n",
        "            eps = 1e-6\n",
        "            y_true = K.reshape(y_true, [w, h])\n",
        "            gt_points = K.cast(tf.where(y_true > 0.5), dtype=tf.float32)\n",
        "            num_gt_points = tf.shape(gt_points)[0]\n",
        "            y_pred = K.flatten(y_pred)\n",
        "            p = y_pred\n",
        "            p_replicated = tf.squeeze(K.repeat(tf.expand_dims(p, axis=-1), num_gt_points))\n",
        "            d_matrix = cdist(all_img_locations, gt_points)\n",
        "            num_est_pts = tf.reduce_sum(p)\n",
        "            term_1 = (1 / (num_est_pts + eps)) * K.sum(p * K.min(d_matrix, 1))\n",
        "\n",
        "            d_div_p = K.min((d_matrix + eps) / (p_replicated ** alpha + (eps / max_dist)), 0)\n",
        "            d_div_p = K.clip(d_div_p, 0, max_dist)\n",
        "            term_2 = K.mean(d_div_p, axis=0)\n",
        "\n",
        "            return term_1 + term_2\n",
        "\n",
        "        batched_losses = tf.map_fn(lambda x:\n",
        "                                   loss(x[0], x[1]),\n",
        "                                   (y_true, y_pred),\n",
        "                                   dtype=tf.float32)\n",
        "        return K.mean(tf.stack(batched_losses))\n",
        "\n",
        "    return hausdorff_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_unet_collection import models, losses\n",
        "model = models.r2_unet_2d((None, None, 3), [16, 32, 64, 128], n_labels=1,\n",
        "                          stack_num_down=2, stack_num_up=1, recur_num=2,\n",
        "                          activation='ReLU', output_activation='Sigmoid',\n",
        "                          batch_norm=True, pool='max', unpool='nearest', name='r2unet')"
      ],
      "metadata": {
        "id": "zl6nDbuMjAn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models"
      ],
      "metadata": {
        "id": "Isu2yjPtSFP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_unet_collection"
      ],
      "metadata": {
        "id": "0Gl4jm0cuXse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision"
      ],
      "metadata": {
        "id": "tTdEElmOGD0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_unet_collection import losses\n",
        "\n",
        "def hybrid_loss(y_true, y_pred):\n",
        "\n",
        "    loss_focal = losses.focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n",
        "    loss_iou = losses.iou_seg(y_true, y_pred)\n",
        "\n",
        "    # (x)\n",
        "    loss_dice = losses.dice(y_true, y_pred)\n",
        "\n",
        "    return loss_focal+loss_iou +loss_dice"
      ],
      "metadata": {
        "id": "MV16LQ6gSJj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow\n",
        "import segmentation_models as sm\n",
        "model.compile(loss = hybrid_loss, optimizer=Adam(lr = 1e-3), metrics=['accuracy',dice_coef,jacard, confusion,tp,tn,recall_m,precision_m])\n",
        "#print(model_Unet.summary())\n",
        "#sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)\n"
      ],
      "metadata": {
        "id": "2ezF7rgN_3J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = len(X_train1)//16\n",
        "val_steps_per_epoch = len(X_test)//16"
      ],
      "metadata": {
        "id": "vlbUJSSjSKE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "\n",
        "#ModelCheckpoint callback saves a model at some interval.\n",
        "filepath=\"/content/drive/MyDrive/Skin/paper/weights/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\" #File name includes epoch and validation accuracy.\n",
        "#Use Mode = max for accuracy and min for loss.\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
        "#early_stop = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n",
        "#This callback will stop the training when there is no improvement in\n",
        "# the validation loss for three consecutive epochs.\n",
        "\n",
        "#CSVLogger logs epoch, acc, loss, val_acc, val_loss\n",
        "log_csv = CSVLogger('/content/drive/MyDrive/Skin/paper/weights/Res_SKIN_logs.csv', separator=',', append=False)\n",
        "\n",
        "callbacks_list = [checkpoint, log_csv]\n"
      ],
      "metadata": {
        "id": "woVHbbS5Hc_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bn_act(x, act=True):\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if act == True:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = bn_act(x)\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
        "    return conv\n",
        "\n",
        "def stem(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
        "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([conv, shortcut])\n",
        "    return output\n",
        "\n",
        "def residual_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([shortcut, res])\n",
        "    return output\n",
        "\n",
        "def upsample_concat_block(x, xskip):\n",
        "    u = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    c = keras.layers.Concatenate()([u, xskip])\n",
        "    return c\n",
        "\n",
        "def ResUNet():\n",
        "    f = [32, 64, 128, 256,512,1024]\n",
        "    inputs = keras.layers.Input((128,128, 3))\n",
        "\n",
        "    ## Encoder\n",
        "    e0 = inputs\n",
        "    e1 = stem(e0, f[0])\n",
        "    e2 = residual_block(e1, f[1], strides=2)\n",
        "    e3 = residual_block(e2, f[2], strides=2)\n",
        "    e4 = residual_block(e3, f[3], strides=2)\n",
        "    e5 = residual_block(e4, f[4], strides=2)\n",
        "    e6 = residual_block(e5, f[5], strides=2)\n",
        "\n",
        "\n",
        "    ## Bridge\n",
        "    b0 = conv_block(e6, f[5], strides=1)\n",
        "    b1 = conv_block(b0, f[5], strides=1)\n",
        "\n",
        "    ## Decoder\n",
        "    u1 = upsample_concat_block(b1, e5)\n",
        "    d1 = residual_block(u1, f[5])\n",
        "\n",
        "    u2 = upsample_concat_block(d1, e4)\n",
        "    d2 = residual_block(u2, f[4])\n",
        "\n",
        "    u3 = upsample_concat_block(d2, e3)\n",
        "    d3 = residual_block(u3, f[3])\n",
        "\n",
        "    u4 = upsample_concat_block(d3, e2)\n",
        "    d4 = residual_block(u4, f[2])\n",
        "\n",
        "    u5 = upsample_concat_block(d4, e1)\n",
        "    d5 = residual_block(u5, f[1])\n",
        "\n",
        "\n",
        "\n",
        "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d5)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Model integration\n",
        "model = models.Model(inputs, conv_final, name=\"ResUNet\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8dge_4obkej9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "batch_size=16\n",
        "start1 = datetime.now()\n",
        "\n",
        "Unet_history = model.fit(X_train1, Y_train1,\n",
        "                    verbose=1,epochs=100,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    validation_steps=val_steps_per_epoch\n",
        "                   )\n",
        "\n",
        "stop1 = datetime.now()\n",
        "#Execution time of the model\n",
        "execution_time_Unet = stop1-start1\n",
        "print(\"MADR execution time is: \", execution_time_Unet)\n",
        "model.save('/content/drive/MyDrive/Skin/weight/consolidate/Res_SKIN_model.h5')"
      ],
      "metadata": {
        "id": "dIF4HzyqcqY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = model()\n",
        "model.load_weights('/content/drive/MyDrive/Skin/weight/consolidate/Res_SKIN_model.h5') #Trained for 50 epochs and then additional 100"
      ],
      "metadata": {
        "id": "tiOqgfIHiWkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tensorflow.keras.utils import normalize\n",
        "test_img_number = 122\n",
        "test_img = X_test_do[test_img_number]\n",
        "ground_truth=Y_test_d0[test_img_number]\n",
        "plt.rcdefaults()\n",
        "test_img_input=np.expand_dims(test_img, 0)\n",
        "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
        "\n"
      ],
      "metadata": {
        "id": "AGdCLhAH7F5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seg=prediction\n",
        "gt=ground_truth[:,:,0]\n",
        "dice = (np.sum(seg[gt==1])*2.0 / (np.sum(seg) + np.sum(gt)))*100\n",
        "dice=round(dice,4)\n",
        "plt.title(f\"Dice score of Att-Unet is {dice}\",fontsize=12)\n",
        "plt.imshow(prediction, cmap='gray')"
      ],
      "metadata": {
        "id": "pphUXX8_7Ouv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}